{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistent Bayes: Some Motivating Examples\n",
    "---\n",
    "\n",
    "Copyright 2017-2018 Michael Pilosov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "_tested with python 3.6 on 01/26/18_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematics and Plotting\n",
    "from HelperFuns import * # pyplot wrapper functions useful for visualizations, numpy, scipy, etc.\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.rcParams['figure.figsize'] = 10, 5\n",
    "from cbayes import sample, solve, distributions\n",
    "# Interactivity\n",
    "from ipywidgets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Formulating the Inverse Problem\n",
    "---\n",
    "### Prior Information/Assumptions\n",
    "\n",
    "* We assume that the true value $\\lambda_0$ belongs to a parameter space $\\Lambda$.\n",
    "\n",
    "\n",
    "* Much like in the classical statistical Bayesian framework, we begin by encapsulating our pre-existing beliefs about our parameters in a distribution in a prior distribution on $\\Lambda$, $\\pi^{prior}_\\Lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Observed Density\n",
    "\n",
    "* The observed density represents the uncertainty in an observation of a measurable quantity of interest map that takes input parameters and produces a vector in $\\mathbb{R}^d$.\n",
    "\n",
    "\n",
    "* While there are problem scenarios you can posit where the observed density corresponds to a normalized likelihood function from the statistical Bayesian approach, the quantity of interest may not necessarily just be the uncertainty in the measurement data. \n",
    "\n",
    "\n",
    "* If the quantity of interest is indeed a single direct measurement, then the likelihood is the observed. For example, for some true parameter $\\lambda_0$ and model $u(\\lambda, t)$, suppose your quantity of interest is defined as a single evaluation at some time $t_0$. The measurement uncertainty contained in that one measurement would constitute your observed density. \n",
    "\n",
    "\n",
    "* However, if we have a collection of observations, such as at $t_0, t_2, \\dots, t_K$,  each of which is polluted by normally distributed noise, a common thing to do from Bayesian and Frequentist statistics would be to minimize the mean-or sum-squared error. If the sum squared error (SSE) is what we treat as our quantity of interest, the observed density on $\\mathcal{D}$, denoted by $\\pi^{obs}_{\\mathcal{D}}(d)$, would be given by a $\\chi^2_{K+1}$ distribution.\n",
    "\n",
    "\n",
    "### The Posterior Density\n",
    "\n",
    "* Let $\\pi^{O(prior)}_{\\mathcal{D}}(d)$ denote the push-forward of the prior density onto $\\mathcal{D}$. Then, the posterior density on $\\Lambda$ is given by\n",
    "\n",
    "$$\n",
    "    \\pi^{post}_\\Lambda(\\lambda) := \\pi^{prior}_\\Lambda(\\lambda)\\frac{\\pi^{obs}_{\\mathcal{D}}(Q(\\lambda))}{\\pi^{O(prior)}_{\\mathcal{D}}(Q(\\lambda))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define your Map\n",
    "_ Choose from one of the following example options, feel free to add your own _ \n",
    "\n",
    "$O_1(\\lambda) = \\sum_{i=1}^n \\lambda_i$  \n",
    "\n",
    "$O_2(\\lambda) = \\lbrace \\lambda_0,\\;\\; \\lambda_0 + \\lambda_1 \\rbrace$ \n",
    "\n",
    "$O_3(\\lambda) = \\lbrace \\lambda_0,\\;\\; \\lambda_0 - \\lambda_1, \\; \\;\\lambda_1^2 \\rbrace$\n",
    "\n",
    "$O_4(\\lambda) = (1-x)^2 + (y - x^2)^2$  ( This is the [Rosenbrock Function](https://en.wikipedia.org/wiki/Rosenbrock_function) with $a=1$ and $b=100$. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PtO_fun_choice = 1\n",
    "\n",
    "def fun1(lam): # sum all params\n",
    "    return np.sum(lam,axis=1)\n",
    "\n",
    "def fun2(lam): # pull two params, linear combination.\n",
    "    return np.array([ lam[:,0] ,lam[:,0]+lam[:,1]]).transpose()\n",
    "\n",
    "def fun3(lam): # pull two params, linear combination.\n",
    "    return np.array([ lam[:,0] ,lam[:,0]-lam[:,1], lam[:,1]**2 ]).transpose()\n",
    "\n",
    "def rosenbrock(lam):\n",
    "     return (1.-lam[:,0])**2 + 100*(lam[:,1]-lam[:,0]**2.)**2\n",
    "    \n",
    "if PtO_fun_choice == 1:\n",
    "    PtO_fun = fun1\n",
    "elif PtO_fun_choice == 2:\n",
    "    PtO_fun = fun2\n",
    "elif PtO_fun_choice == 3:\n",
    "    PtO_fun = fun3\n",
    "elif PtO_fun_choice == 4:\n",
    "    PtO_fun = rosenbrock\n",
    "else:\n",
    "    raise( ValueError('Specify Proper PtO choice!') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Sample from $\\Lambda$\n",
    "_Here we implement uniform random priors on the unit hypercube_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2 # Specify input space dimension (n)\n",
    "num_samples = int(1E3) # number of input samples (N)\n",
    "s_set = sample.sample_set(size=(num_samples, input_dim))\n",
    "\n",
    "if PtO_fun_choice == 1:\n",
    "    s_set.set_dist('normal', {'loc': 0, 'scale': 1})\n",
    "elif PtO_fun_choice == 2:\n",
    "    s_set.set_dist('normal', {'loc': -1, 'scale': 2})\n",
    "elif PtO_fun_choice == 3:\n",
    "    s_set.set_dist('normal', {'loc': 0, 'scale': 1})\n",
    "elif PtO_fun_choice == 4: # rosenbrock\n",
    "    s_set.set_dist('uniform', {'loc': [-1, -1], 'scale': [2, 1]})\n",
    "                   \n",
    "s_set.generate_samples()\n",
    "\n",
    "lam = s_set.samples # create a pointer for ease of reference later with plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interactive(pltdata, data = fixed(lam), inds = fixed(None), \n",
    "                    N = widgets.IntSlider(value=500, min = 100, max=5000, step=100, continuous_update=False), \n",
    "                    eta_r = fixed(None), space=fixed(0.05), svd=widgets.Checkbox(value=False), color=widgets.Text(value=\"orange\"),\n",
    "                    view_dim_1 = widgets.IntSlider(value=0, min=0, max=input_dim-1, step=1, continuous_update=False), \n",
    "                    view_dim_2 = widgets.IntSlider(value=input_dim-1, min=0, max=input_dim-1, step=1, continuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Compute Data Space $O(\\Lambda) = \\mathcal{D}$ \n",
    "\n",
    "Format: `(n_dims, n_samples)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_set = sample.map_samples_and_create_problem(s_set, PtO_fun)\n",
    "D = p_set.output.samples\n",
    "\n",
    "# this is how we handle trying to infer the dimension based on what the map put out.\n",
    "try:\n",
    "    output_dim = D.shape[1] # if your function was coded correctly, you should have an (n, d) data space.\n",
    "except IndexError:\n",
    "    print(Warning(\"Warning: Your map might be returning the wrong dimensional data.\"))\n",
    "    try:\n",
    "       output_dim = D.shape[0] \n",
    "    except IndexError:\n",
    "        print(Warning(\"Warning: Guessing it's 1-dimensional.\"))\n",
    "        output_dim = 1\n",
    "print('dimensions :  lambda = '+str(lam.shape)+'   D = '+str(D.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Push-Forward of the Prior $P_{O(\\Lambda)}$\n",
    "_ ... i.e. Characterize the Data Space_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Marginal Visualization\n",
    "p_set.compute_pushforward_dist()\n",
    "pf_dist = p_set.pushforward_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "widgets.interactive(pltdata, data = fixed(pf_dist), inds = fixed(None), \n",
    "        N = widgets.IntSlider(value=500, min = 100, max=5000, step=100, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=fixed(False), color=widgets.Text(value=\"brown\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=output_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=output_dim-1, min=0, max=output_dim-1, step=1, continuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Observed Probability Measure $P_\\mathcal{D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PtO_fun_choice == 4:\n",
    "    p_set.set_observed_dist('normal', {'loc': 100, 'scale': 12}) # FOR ROSENBROCK\n",
    "\n",
    "elif PtO_fun_choice == 3:\n",
    "#         p_set.set_observed_dist('normal', {'loc':[0, 0, 0], 'scale':[0.5, 0.25, 1]}) # better for function choice = 2\n",
    "    p_set.set_observed_dist('uniform', {'loc':[-0.5, -0.5, -0.5], 'scale':[1, 1, 1]}) # better for function choice = 2\n",
    "\n",
    "elif PtO_fun_choice == 2:\n",
    "    p_set.set_observed_dist('normal', {'loc':[0, 0], 'scale':[1, 1]}) # default is normal based on the data space # for function choice = 1\n",
    "\n",
    "elif PtO_fun_choice == 1:\n",
    "    p_set.set_observed_dist('uni', {'loc':0, 'scale':0.3}) # default is normal based on the data space # for function choice = 1\n",
    "\n",
    "obs_dist = p_set.observed_dist # this is define a pointer for ease of reference.\n",
    "\n",
    "widgets.interactive(pltdata, data = fixed(obs_dist), inds = fixed(None), \n",
    "        N = widgets.IntSlider(value=500, min = 100, max=5000, step=100, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=fixed(False), color=widgets.Text(value=\"wine\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=output_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=output_dim-1, min=0, max=output_dim-1, step=1, continuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "At this point we have performed the computations we need to. We have evaluated the input points through our map and performed a KDE on them. It would be useful at this point to save this object and/or its evaluation at every point in the data space for later re-use. Doing so here would be an appropriate place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Accept/Reject Sampling of Posterior\n",
    "\n",
    "Since we have already used the samples in our prior to compute the pushforward density, we can re-use these with an accept/reject algorithm to get a set of samples generated from the posterior according to the solution of the stochastic inverse problem as outlined in the Consistent Bayes formulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_set.set_ratio()\n",
    "eta_r = p_set.ratio\n",
    "solve.problem(p_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_inds = p_set.accept_inds\n",
    "lam_accept = p_set.input.samples[accept_inds,:]\n",
    "num_accept = len(accept_inds)\n",
    "print('Number accepted: %d = %2.2f%%'%(num_accept, 100*np.float(num_accept)/num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Posterior Density\n",
    "### (Visualize Accept/Reject Samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "widgets.interactive(pltdata, data = fixed(lam), inds = fixed(accept_inds), \n",
    "        N = widgets.IntSlider(value=num_accept/2, min = 2, max=num_accept+1, step=1, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=widgets.Checkbox(value=False), color=widgets.Text(value=\"orange\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=input_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=input_dim-1, min=0, max=input_dim-1, step=1, continuous_update=False))\n",
    "\n",
    "# You will visualize the accepted samples in a subset of size N of the input samples. \n",
    "# This is mostly for faster plotting, but also so you can see the progression of accepted sampling in the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Now what? \n",
    "\n",
    "Well, we can...\n",
    "\n",
    "## _Visualize the Quality of our SIP Solution by Comparing it to the Observed_\n",
    "_We compare the push-forward of the posterior using accepted samples against the observed density_  \n",
    "_(SIP = Stochastic Inverse Problem)_\n",
    "### Observed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "widgets.interactive(pltdata, data = fixed(obs_dist), inds = fixed(None), \n",
    "        N = widgets.IntSlider(value=500, min = 100, max=5000, step=100, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=fixed(False), color=widgets.Text(value=\"wine\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=output_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=output_dim-1, min=0, max=output_dim-1, step=1, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushforward of Posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interactive(pltdata, data = fixed(D), inds = fixed(accept_inds), \n",
    "        N = widgets.IntSlider(value=num_accept/2, min = 2, max=num_accept-1, step=1, continuous_update=False), \n",
    "        eta_r = fixed(None), space=fixed(0.05), svd=fixed(False), color=widgets.Text(value=\"eggplant\"),\n",
    "        view_dim_1 = widgets.IntSlider(value=0, min=0, max=output_dim-1, step=1, continuous_update=False), \n",
    "        view_dim_2 = widgets.IntSlider(value=output_dim-1, min=0, max=output_dim-1, step=1, continuous_update=False))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
