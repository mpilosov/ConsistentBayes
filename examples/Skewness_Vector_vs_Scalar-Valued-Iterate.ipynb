{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbayes.sample\n",
    "import cbayes.distributions\n",
    "import cbayes.solve\n",
    "import numpy as np\n",
    "import ipywidgets as wd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sstats\n",
    "import scipy.spatial as spat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following linear map $Q_s: \\mathbb{R}^2 \\to \\mathbb{R}^2$ is defined to have skewness $s$ at all $\\lambda \\in \\Lambda$.  \n",
    "\n",
    "$$\n",
    "Q_s(\\lambda) = \\lbrace \\, \\lambda_1, \\; \\lambda_1 \\sqrt{s^2 - 1} + \\lambda_2 \\, \\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Identities\n",
    "Let $\\lambda$ denote an arbitrary Gaussian random variable with mean $\\mu_\\lambda$ and covariance $\\Sigma_\\lambda$,\n",
    "$$\n",
    "\\lambda \\sim N\\left(\\mu_\\lambda, \\Sigma_\\lambda\\right).\n",
    "$$\n",
    "Then, for a matrix $A$, \n",
    "$$\n",
    "A\\lambda \\sim N\\left(A\\mu_\\lambda,\\, A\\Sigma_\\lambda A^T\\right)\n",
    "$$\n",
    "Let $\\eta = A\\lambda + e$, where $e\\sim N(0,\\Sigma_e)$,   \n",
    "then the posterior $p(\\lambda | \\eta)$ is given by\n",
    "\n",
    "$$\n",
    "p(\\lambda | \\eta=\\bar{\\eta}) = N\\left(\\hat{\\mu}, \\hat{\\Sigma}\\right),\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\hat{\\mu} = \\mu_\\lambda + \\Sigma_\\lambda A^T\\left(A\\Sigma_\\lambda A^T + \\Sigma_e\\right)^{-1}\\left(\\bar{\\eta} - A\\mu_\\lambda\\right)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\hat{\\Sigma} = \\Sigma_\\lambda - \\Sigma_\\lambda A^T\\left(A\\Sigma_\\lambda A^T + \\Sigma_e\\right)^{-1}A\\Sigma_\\lambda\n",
    "$$\n",
    "\n",
    "[These notes](https://cs.nyu.edu/~roweis/notes/gaussid.pdf) by Sam Roweis also provide some useful identities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function that generates an arbitrarily ill-condidtioned 2-2 map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(skew, tile=1):\n",
    "    # this function makes a linear map whos first component is the x-unit vector\n",
    "    # and each subsequent component is a norm-1 vector satisfying the property\n",
    "    # that the 2-2 map made from it and the aforementioned unit vector is a map\n",
    "    # with skewness in skew_range, which is a list of desired skewnesses   \n",
    "    # TODO currently this map only works for 2-D input space     \n",
    "    \n",
    "    def my_model(parameter_samples):\n",
    "        Q_map = skewmat(skew, tile)\n",
    "#         QoI_samples = Q_map@parameter_samples\n",
    "        QoI_samples = np.dot(parameter_samples, np.transpose(Q_map))\n",
    "#         QoI_samples = Q_map@parameter_samples.T\n",
    "        return QoI_samples\n",
    "    return my_model\n",
    "\n",
    "def skewmat(skew, tile=1):\n",
    "    Q_map = [ [1.0, 0.0] ] # all map components have the same norm, rect_size to have measures of events equal btwn spaces.\n",
    "    Q_map.append( [np.sqrt(skew**2 - 1), 1] ) # taken with the first component, this leads to a 2-2 map with skewsness 's'\n",
    "    Q_map = np.array( Q_map )\n",
    "    Q_map = np.tile(Q_map.T, tile).T\n",
    "    return Q_map\n",
    "\n",
    "def gauss_sol(prior_mean, prior_std, data_std, A, data):\n",
    "    if type(prior_mean) is int:\n",
    "        prior_mean = [prior_mean, prior_mean]\n",
    "    if type(prior_mean) is float:\n",
    "        prior_mean = [prior_mean, prior_mean]\n",
    "    if type(prior_mean) is list:\n",
    "        prior_mean = np.array(prior_mean).reshape(-1,1)\n",
    "    if type(prior_std) is list:\n",
    "        prior_std = np.array(prior_std).reshape(-1,1)\n",
    "    if type(data_std) is list:\n",
    "        data_std = np.array(data_std).reshape(-1,1)\n",
    "    prior_cov = prior_std*prior_std*np.eye(2) \n",
    "    data_cov = data_std*data_std*np.eye(len(data_std)) \n",
    "    \n",
    "    ASA = A@prior_cov@A.T\n",
    "    \n",
    "    precision = np.linalg.inv(ASA + data_cov)\n",
    "    kahlman_update = (prior_cov@A.T@precision)\n",
    "    post_mean = prior_mean + kahlman_update@(data - A@prior_mean)\n",
    "    post_cov = prior_cov - kahlman_update@A@prior_cov\n",
    "    \n",
    "    return prior_mean, prior_cov, post_mean, post_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = skewmat(1.01)\n",
    "# # print(A)\n",
    "# data_std = 0.25\n",
    "# prior_std = 1\n",
    "# prior_mean = 0\n",
    "# lam_true = np.array([0.0, 0.0])\n",
    "# obs_data = A@lam_true.T + data_std*np.random.randn(2)\n",
    "\n",
    "# prior_mean, prior_cov, post_mean, post_cov = gauss_sol(prior_mean, prior_std, data_std, A, obs_data.reshape(-1,1) )\n",
    "# print(post_mean.T,'\\n')\n",
    "# print(post_cov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the contours of this vector-valued map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_generator(model, obs_data, sigma):   # this generates a sum of squared residuals.\n",
    "    M = len(obs_data)\n",
    "    noise = np.random.randn(M)*sigma\n",
    "    def QoI_fun(inputs): # that conforms to our desired model input\n",
    "        M = len(obs_data)\n",
    "#         N = inputs.shape[0]\n",
    "        predictions = model(inputs)\n",
    "        assert predictions.shape[1] == M\n",
    "        residuals = predictions - obs_data + noise*0\n",
    "        QoI = (1./M)*np.sum( (residuals/sigma)**2, axis=1 )  # MRSE\n",
    "        return QoI\n",
    "    return QoI_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate samples and map them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(num_samples=5000, skew=1, prior_x=0.0, prior_y=0.0, \n",
    "            prior_std = 1.0, data_std = 0.25, trials=1, \n",
    "            color_map = 'jet', num_levels = 40, seed=12, show_accept=False):\n",
    "    fsize=16 # font size\n",
    "    vmin, vmax = 0, None # height bounds for filled contours\n",
    "    pbound = 1.0 # parameter space bounds\n",
    "    dbound = 1.0  # data space bound\n",
    "    prior_alpha = 0.15 # transparency of prior contours\n",
    "    true_posterior_alpha = 0.2 # transparency for the contours of the true solution\n",
    "    \n",
    "    num_plot_pts = 100 # resoltion for 1-D plots\n",
    "    show_kahlman = True\n",
    "    show_slice = False # messy - show slice through middle of axis to interrogate posterior\n",
    "    normalize_marginals = False # normalizing the marginals (dividing the )\n",
    "    num_observations = trials*2\n",
    "    \n",
    "    if normalize_marginals:\n",
    "        maxht = 0.15 # max y-axis height\n",
    "        vline_ht = 0.05 # height of vertical bar\n",
    "    else:\n",
    "        maxht = 150\n",
    "        vline_ht = 100 # shows truth\n",
    "    print(\"Working....\")\n",
    "    model = make_model(skew, trials)\n",
    "    lam_true = np.array([0.25, 0.25])\n",
    "    obs_data = model(lam_true)\n",
    "    \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    obs_data_noisy = obs_data + data_std*np.random.randn(num_observations)\n",
    "#     print('data:', obs_data_noisy)\n",
    "    # ANALYTICAL SOLUTION\n",
    "    prior_mean = np.array([prior_x, prior_y])\n",
    "    A = skewmat(skew, trials)\n",
    "    prior_mean, prior_cov, post_mean, post_cov = gauss_sol(prior_mean, \n",
    "                                                    prior_std, np.ones(num_observations)*data_std, A, obs_data_noisy)\n",
    "    print(post_cov)\n",
    "    post_pred = model(post_mean)\n",
    "    \n",
    "    \n",
    "    ########## DEFINE MSE ####################################################################################################\n",
    "    mse_fun = MSE_generator(model, obs_data_noisy, data_std)\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    s_input_set = cbayes.sample.sample_set(size=(num_samples, 2))\n",
    "    s_input_set.set_dist(dim=0, distribution='normal', kwds={'loc': prior_mean[0], 'scale': prior_std})\n",
    "    s_input_set.set_dist(dim=1, distribution='normal', kwds={'loc': prior_mean[1], 'scale': prior_std})\n",
    "    input_samples = s_input_set.generate_samples(seed=seed)\n",
    "    \n",
    "    modelV = make_model(skew, 1)\n",
    "    output_samples_vector_valued = modelV(input_samples)\n",
    "    \n",
    "\n",
    "    #### VECTOR PROBLEM\n",
    "    s_output_set_vector_valued = cbayes.sample.sample_set(size=(num_samples, 2))\n",
    "    s_output_set_vector_valued.samples = output_samples_vector_valued\n",
    "    \n",
    "    p_set_vector = cbayes.sample.problem_set(s_input_set, s_output_set_vector_valued)\n",
    "    \n",
    "    # Set observed \n",
    "    obs_data_reshape = obs_data_noisy.reshape(trials,2) # compute mean\n",
    "    mean_data = np.mean(obs_data_reshape,axis=0)\n",
    "    print('mean data:', mean_data)\n",
    "    \n",
    "    p_set_vector.set_observed_dist(dim=0, dist='normal', \n",
    "                                   kwds={'loc': mean_data[0], 'scale': data_std})\n",
    "    p_set_vector.set_observed_dist(dim=1, dist='normal', \n",
    "                                   kwds={'loc': mean_data[1], 'scale': data_std})\n",
    "    \n",
    "    p_set_vector.model = make_model(skew, 1)\n",
    "    \n",
    "    p_set_vector.compute_pushforward_dist()\n",
    "    p_set_vector.set_ratio()\n",
    "    \n",
    "    cbayes.solve.problem(p_set_vector)\n",
    "    accepted_inputs_vector = p_set_vector.input.samples[p_set_vector.accept_inds,:]\n",
    "    print('num accepted for vector-valued:', len(accepted_inputs_vector), \n",
    "          'mean: %2.4f, %2.4f'%(np.mean(accepted_inputs_vector[:,0]), np.mean(accepted_inputs_vector[:,1])), \n",
    "          'sd: %2.4f, %2.4f'%(np.std(accepted_inputs_vector[:,0]), np.std(accepted_inputs_vector[:,1])) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### SCALAR PROBLEM\n",
    "#             randtrials = 10\n",
    "#     for _ in range(randtrials):\n",
    "    #     mse_fun = cbayes.sample.MSE_generator(model, obs_data_noisy, data_std)\n",
    "\n",
    "    \n",
    "    \n",
    "    output_samples_scalar_valued = mse_fun(input_samples)\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    s_output_set_scalar_valued = cbayes.sample.sample_set(size=(num_samples, 1))\n",
    "    s_output_set_scalar_valued.samples = output_samples_scalar_valued\n",
    "    p_set_scalar = cbayes.sample.problem_set(s_input_set, s_output_set_scalar_valued)\n",
    "    \n",
    "    \n",
    "    # Set observed \n",
    "    p_set_scalar.set_observed_dist('gamma', {'a':num_observations/2.0, 'scale':2.0/num_observations}, dim=0)\n",
    "    p_set_scalar.model = mse_fun\n",
    "    \n",
    "    p_set_scalar.compute_pushforward_dist(method='sk', kwds={'bandwidth': 0.1}) ############################################################################\n",
    "#     p_set_scalar.compute_pushforward_dist()\n",
    "    p_set_scalar.set_ratio()\n",
    "    \n",
    "    cbayes.solve.problem(p_set_scalar)\n",
    "    accepted_inputs_scalar = p_set_scalar.input.samples[p_set_scalar.accept_inds,:]\n",
    "    print('num accepted for scalar-valued:', len(accepted_inputs_scalar), \n",
    "          'mean: %2.4f, %2.4f'%(np.mean(accepted_inputs_scalar[:,0]), np.mean(accepted_inputs_scalar[:,1])), \n",
    "          'sd: %2.4f, %2.4f'%(np.std(accepted_inputs_scalar[:,0]), np.std(accepted_inputs_scalar[:,1])) )\n",
    "    \n",
    "    \n",
    "    ########## PLOTTING ################\n",
    "#     fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(15,15))\n",
    "    fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(15,10))\n",
    "    # handles if you want to reorient figures \n",
    "#     vv_contours = axs[0,0] # first option \n",
    "#     vs_contours = axs[0,1]\n",
    "#     vv_data_1 = axs[0,2]\n",
    "#     vv_data_2 = axs[1,2]\n",
    "#     vv_post = axs[1,0]\n",
    "#     vs_post = axs[1,1]\n",
    "#     vv_post_marg = axs[2,0]\n",
    "#     vs_post_marg = axs[2,1]    \n",
    "#     vs_pf = axs[2,2]\n",
    "\n",
    "#     vv_contours = axs[1,1] # second option \n",
    "#     vv_data_1 = axs[0,0]\n",
    "#     vv_data_2 = axs[0,1]\n",
    "#     vv_post_marg = axs[1,0] # fills upper left quadrant\n",
    "    \n",
    "#     vs_contours = axs[1,2]\n",
    "#     vs_pf = axs[0,2]\n",
    "    \n",
    "#     vv_post = axs[2,1]\n",
    "#     vs_post = axs[2,2]\n",
    "    \n",
    "#     vs_post_marg = axs[2,0]    \n",
    "    \n",
    "#     vv_contours = axs[1,0] # third option \n",
    "#     vv_data_1 = axs[0,0]\n",
    "#     vv_data_2 = axs[0,2]\n",
    "#     vv_post_marg = axs[0,1] # fills upper left quadrant\n",
    "\n",
    "#     vs_contours = axs[1,2]\n",
    "#     vs_pf = axs[2,2]\n",
    "\n",
    "#     vv_post = axs[2,0]\n",
    "#     vs_post = axs[2,1]\n",
    "\n",
    "#     vs_post_marg = axs[1,1]   \n",
    "    \n",
    "    vv_contours = axs[0,0] # fourth option \n",
    "\n",
    "    vs_contours = axs[0,2]\n",
    "    vs_pf = axs[1,2]\n",
    "\n",
    "    vv_post = axs[1,0]\n",
    "    vs_post = axs[1,1]\n",
    "\n",
    "    vs_post_marg = axs[0,1]   \n",
    "    \n",
    "    \n",
    "    x = input_samples[:,0] \n",
    "    y = input_samples[:,1]\n",
    "    xs = accepted_inputs_scalar[:,0]\n",
    "    ys = accepted_inputs_scalar[:,1]\n",
    "\n",
    "    xv = accepted_inputs_vector[:,0]\n",
    "    yv = accepted_inputs_vector[:,1]\n",
    "    \n",
    "    \n",
    "    # CONTOURS FOR FORWARD MAP\n",
    "    vv_contours.tricontour(x, y, output_samples_vector_valued[:,0], num_levels, cmap=color_map,\n",
    "                          vmin=vmin, vmax=vmax)\n",
    "    vv_contours.tricontour(x, y, output_samples_vector_valued[:,1], num_levels, cmap=color_map,\n",
    "                          vmin=vmin, vmax=vmax)\n",
    "    vs_contours.tricontour(x, y, output_samples_scalar_valued, num_levels, cmap=color_map,\n",
    "                          vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    vv_contours.set_title('Parameter Space with Contours\\nof Vector-Valued QoI Map', fontsize=fsize)\n",
    "    vs_contours.set_title('Parameter Space with Contours\\nof Scalar-Valued QoI Map', fontsize=fsize)\n",
    "    \n",
    "    \n",
    "    # MESH PLOT\n",
    "    vpost = p_set_vector.ratio*s_input_set.dist.pdf(input_samples)\n",
    "    spost = p_set_scalar.ratio*s_input_set.dist.pdf(input_samples)\n",
    "    vv_post.tricontourf(x, y, vpost, int(num_levels/2), cmap=color_map,\n",
    "                          vmin=vmin, vmax=vmax)\n",
    "    vs_post.tricontourf(x, y, spost, int(num_levels/2), cmap=color_map,\n",
    "                          vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    # CONTOURS OF KAHLMAN POSTERIOR\n",
    "    post_pdf = sstats.multivariate_normal.pdf(input_samples, mean=post_mean, cov=post_cov, allow_singular=True)\n",
    "    prior_pdf = sstats.multivariate_normal.pdf(input_samples, mean=prior_mean, cov=prior_cov, allow_singular=True)\n",
    "\n",
    "    ####  SHOW KAHLMAN GAIN SOL ####\n",
    "    if show_kahlman:\n",
    "        vv_post.tricontour(x, y, post_pdf, int(num_levels/2), cmap='Greys', \n",
    "                            vmin=vmin, vmax=vmax, alpha=true_posterior_alpha)\n",
    "        vs_post.tricontour(x, y, post_pdf, int(num_levels/2), cmap='Greys', \n",
    "                        vmin=vmin, vmax=vmax, alpha=true_posterior_alpha)\n",
    "    \n",
    "    # PRIOR CONTOURS\n",
    "    vv_contours.tricontour(x, y, prior_pdf, int(num_levels/2), cmap='Greys', \n",
    "                        vmin=vmin, vmax=vmax, alpha=prior_alpha)\n",
    "    vs_contours.tricontour(x, y, prior_pdf, int(num_levels/2), cmap='Greys', \n",
    "                        vmin=vmin, vmax=vmax, alpha=prior_alpha)\n",
    "    \n",
    "    vv_post.set_title('Posterior Distribution for\\nVector-Valued Approach', fontsize=fsize)\n",
    "    vs_post.set_title('Posterior Distribution for\\nScalar-Valued Approach', fontsize=fsize)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ERRORS \n",
    "    print('\\nmean of ratio:', np.mean(p_set_scalar.ratio))\n",
    "    \n",
    "    if num_observations == 0: # analytically, this should be the solution that the scalar version is matching\n",
    "        post_pdf_v = sstats.multivariate_normal.pdf(output_samples_vector_valued, mean=mean_data, \n",
    "                                                  cov=data_std*data_std)\n",
    "        mse_fun2 = cbayes.sample.MSE_generator(model, obs_data_noisy - A.T@A@prior_mean, prior_std*data_std)\n",
    "        output_samples_scalar_valued2 = mse_fun2(input_samples)\n",
    "        post_pdf_s = (prior_pdf*post_pdf_v)/ \\\n",
    "        ( sstats.distributions.chi2.pdf(  output_samples_scalar_valued2, 2) )\n",
    "        \n",
    "        vdist = np.linalg.norm(vpost-post_pdf_v,1)/num_samples\n",
    "        sdist = np.linalg.norm(spost-post_pdf_s,1)/num_samples\n",
    "        bdist = np.linalg.norm(spost-vpost,1)/num_samples\n",
    "        vv_post.tricontour(x, y, post_pdf_v, int(num_levels/2), cmap='Greys', \n",
    "                            vmin=vmin, vmax=vmax, alpha=true_posterior_alpha)\n",
    "        vs_post.tricontour(x, y, post_pdf_s, int(num_levels/2), cmap='Greys', \n",
    "                        vmin=vmin, vmax=vmax, alpha=true_posterior_alpha)\n",
    "#         vs_post.tricontour(x, y, post_pdf, int(num_levels/2), cmap='YlOrRd', \n",
    "#                         vmin=vmin, vmax=vmax, alpha=0.5)\n",
    "    else:\n",
    "        vdist = np.linalg.norm(vpost-post_pdf,1)/num_samples\n",
    "        sdist = np.linalg.norm(spost-post_pdf,1)/num_samples\n",
    "        bdist = np.linalg.norm(spost-vpost,1)/num_samples\n",
    "#     print('L-1 error: %2.2e vector | %2.2e scalar | %2.2e each'%(vdist, sdist, bdist))\n",
    "    \n",
    "    \n",
    "    # PLOT PUSH-FORWARD AND OBSERVED FOR SCALAR MAP\n",
    "    \n",
    "    xx = np.linspace(0, 5, num_plot_pts)\n",
    "    vs_pf.plot(xx, p_set_scalar.pushforward_dist.pdf(xx), label='Push-forward of Prior')\n",
    "    vs_pf.plot(xx, p_set_scalar.observed_dist.pdf(xx), label='Observed (Gamma) Density')\n",
    "    vs_pf.legend()\n",
    "#     vs_pf.set_title('Data Space for\\n$q = [(q_1-o_1)^2 + (q_2-o_2)^2]/2\\sigma_e^2$', fontsize=fsize)\n",
    "    vs_pf.set_title('Data Space for MSE QoI with %0004d trials'%trials)\n",
    "    \n",
    "    # PLOT DATA FOR VECTORS\n",
    "#     xx = np.linspace(-dbound, dbound, num_plot_pts)\n",
    "#     vv_data_1.plot(xx, sstats.distributions.norm.pdf(xx,loc=mean_data[0],scale=data_std), \n",
    "#                   color='blue', ls='-', label='$q_1$')\n",
    "#     vv_data_2.plot(xx, sstats.distributions.norm.pdf(xx,loc=mean_data[1], scale=data_std), \n",
    "#                   color='orange', ls='--', label='$q_2$')    \n",
    "    \n",
    "    \n",
    "    # PLOTTING MARGINALS (both on same axis)\n",
    "    xxx = np.linspace(-pbound, pbound,num_plot_pts)\n",
    "    XX, YY = np.meshgrid(xxx,xxx)\n",
    "    lam = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    zzs = p_set_scalar.evaluate_posterior(lam)\n",
    "    zzs = zzs.reshape(num_plot_pts,num_plot_pts)\n",
    "    if normalize_marginals:\n",
    "        zzs = zzs/np.sum(zzs)\n",
    "    marg_sx = np.sum(zzs, axis=0)\n",
    "    marg_sy = np.sum(zzs, axis=1)\n",
    "    line0, = vs_post_marg.plot(xxx, marg_sx,  color='blue', ls='-', lw=2, label='$\\\\lambda_1$ Marginal')\n",
    "    line1, = vs_post_marg.plot(xxx, marg_sy, color='orange', ls='--', lw=2, label='$\\\\lambda_2$ Marginal')\n",
    "    vs_post_marg.legend()\n",
    "    vs_post_marg.set_title('Posterior Marginals for\\nScalar-Valued Approach', fontsize=fsize)\n",
    "    \n",
    "#     zzv = p_set_vector.evaluate_posterior(lam)\n",
    "#     zzv = zzv.reshape(num_plot_pts,num_plot_pts)\n",
    "#     if normalize_marginals:\n",
    "#         zzv = zzv/np.sum(zzv)\n",
    "#     marg_vx = np.sum(zzv, axis=0)\n",
    "#     marg_vy = np.sum(zzv, axis=1)\n",
    "#     line3, = vv_post_marg.plot(xxx, marg_vx,  color='blue', ls='-',  lw=2, label='$\\\\lambda_1$ Marginal')\n",
    "#     line4, = vv_post_marg.plot(xxx, marg_vy, color='orange', ls='--', lw=2, label='$\\\\lambda_2$ Marginal')\n",
    "#     vv_post_marg.legend()\n",
    "#     vv_post_marg.set_title('Posterior Marginals for\\nVector-Valued Approach', fontsize=fsize)\n",
    "    \n",
    "    \n",
    "        \n",
    "    # slices through middle\n",
    "    if show_slice:\n",
    "        lines1, = vs_post_marg.plot(xxx, zzs[int(num_plot_pts/2),:], '-', lw=3) # lam 1\n",
    "        lines2, = vs_post_marg.plot(xxx, zzs[:,int(num_plot_pts/2)], ':', lw=3) # lam 2\n",
    "        linev1, = vv_post_marg.plot(xxx, zzv[int(num_plot_pts/2),:], '-', lw=3) # lam 1\n",
    "        linev2, = vv_post_marg.plot(xxx, zzv[:,int(num_plot_pts/2)], ':', lw=3) # lam 2\n",
    "        maxht = 10\n",
    "    \n",
    "    ##### ANNOTATIONS \n",
    "    rht, kht = 0.15, 2.5 # red height, black height (for vertical lines)\n",
    "    blw, rlw, klw = 5, 1, 1 # red line weight, black line weight\n",
    "    # SCATTERPLOT ACCEPTED SAMPLES\n",
    "    if show_accept:\n",
    "        vv_post.scatter(xv, yv, color='w', alpha=0.1)\n",
    "        vs_post.scatter(xs, ys, color='w', alpha=0.1)\n",
    "\n",
    "    # VERTICAL LINE FOR TRUE VALUE\n",
    "#     vv_post_marg.vlines(lam_true[0],0,vline_ht, 'k', lw=1)\n",
    "#     vv_post_marg.vlines(lam_true[1],0,vline_ht, 'k', lw=1)\n",
    "#     vs_post_marg.vlines(lam_true[0],0,vline_ht, 'k', lw=1)\n",
    "#     vs_post_marg.vlines(lam_true[1],0,vline_ht, 'k', lw=1)\n",
    "#     vs_post_marg.axis([-pbound, pbound, 0, maxht])\n",
    "#     vv_post_marg.axis([-pbound, pbound, 0, maxht])\n",
    "    \n",
    "    # DATA SPACE VISUALS\n",
    "#     vv_data_1.vlines(mean_data[0], 0, 2*rht, 'brown', lw=blw)\n",
    "#     vv_data_2.vlines(mean_data[1], 0, 2*rht, 'brown', lw=blw) \n",
    "#     for i in range(trials): \n",
    "#         vv_data_1.vlines(obs_data_reshape[i,0], 0, rht, 'r', lw=rlw, alpha=0.25)\n",
    "#         vv_data_2.vlines(obs_data_reshape[i,1], 0, rht, 'r', lw=rlw, alpha=0.25) \n",
    "        \n",
    "#     vv_data_1.vlines(obs_data[0], 0, kht, 'k', lw=klw)\n",
    "#     vv_data_2.vlines(obs_data[1], 0, kht, 'k', lw=klw)\n",
    "#     vv_data_2.set_title('Data Space for $q_2$', fontsize=fsize)\n",
    "#     vv_data_1.set_title('Data Space for $q_1$', fontsize=fsize)\n",
    "    \n",
    "    # SHOW TRUTH AS WHITE DOT WITH BLACK BORDER\n",
    "    vv_post.scatter(lam_true[0], lam_true[1], 60, 'k')\n",
    "    vs_post.scatter(lam_true[0], lam_true[1], 60, 'k')\n",
    "    vv_post.scatter(lam_true[0], lam_true[1], 40, 'w')\n",
    "    vs_post.scatter(lam_true[0], lam_true[1], 40, 'w')\n",
    "    cb_mean_x = np.mean(accepted_inputs_scalar[:,0])\n",
    "    cb_mean_y = np.mean(accepted_inputs_scalar[:,1])\n",
    "    vs_post.scatter(cb_mean_x, cb_mean_y, 20, 'w')\n",
    "    \n",
    "    # FIX AXES TO BE THE SAME\n",
    "    vv_post.axis([-pbound, pbound, -pbound, pbound])\n",
    "    vs_post.axis([-pbound, pbound, -pbound, pbound])\n",
    "    vv_contours.axis([-pbound, pbound, -pbound, pbound])\n",
    "    vs_contours.axis([-pbound, pbound, -pbound, pbound])\n",
    "#     vv_data_1.axis([-dbound, dbound, 0, 1.5])  # normal distributions.\n",
    "#     vv_data_2.axis([-dbound, dbound, 0, 1.5])\n",
    "    \n",
    "    ### SCATTER PLOT TRUTH AND OBSERVED DATA\n",
    "    vv_contours.scatter(obs_data_reshape[:,0], obs_data_reshape[:,1], 50, 'r', alpha=0.4) # plot noisy data to compare\n",
    "    vs_contours.scatter(obs_data_reshape[:,0], obs_data_reshape[:,1], 50, 'r', alpha=0.4)\n",
    "    ## EMPHASIZE LAST SAMPLE\n",
    "    vv_contours.scatter(obs_data_reshape[-1,0], obs_data_reshape[-1,1], 100, 'g', alpha=0.8) # plot noisy data to compare\n",
    "    vs_contours.scatter(obs_data_reshape[-1,0], obs_data_reshape[-1,1], 100, 'g', alpha=0.8)\n",
    "    vs_contours.scatter(post_pred[0], post_pred[1], 100, 'b', alpha=0.8)\n",
    "    \n",
    "    vv_contours.scatter(lam_true[0], lam_true[1], 100, 'k') # plot true input to compare\n",
    "    vs_contours.scatter(lam_true[0], lam_true[1], 100, 'k')\n",
    "    \n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "#     for ax in axs.flat:\n",
    "    vv_contours.set_xticklabels([])\n",
    "    vv_contours.set_yticklabels([])\n",
    "#     vs_contours.set_xticklabels([])\n",
    "#     vs_contours.set_yticklabels([])\n",
    "#     vs_post.set_xticklabels([])\n",
    "#     vs_post.set_yticklabels([])\n",
    "#     vs_post.label_outer()\n",
    "    plt.savefig('skew/finalassimilating_pts_seed%d_%0004d.png'%(seed,trials), dpi=40 )\n",
    "    plt.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormaps = ['viridis', 'plasma', 'inferno', 'magma', 'jet', \n",
    "             'Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "            'YlOrBr', 'YlOrRd', 'PuRd', 'RdPu', 'BuPu', 'GnBu', 'YlGnBu']\n",
    "\n",
    "out = wd.interactive(compare, \n",
    "            num_samples = wd.IntSlider(value=1000, min=100, max=10000, step=100, continuous_update=False, description=r'N'), \n",
    "            skew = wd.FloatSlider(value=1.0, min=1.0, max=2.0, step=0.05, continuous_update=False, description=r'skewness', orientation='horizontal'), \n",
    "            prior_x = wd.FloatSlider(value=0.0, min=-0.25, max=0.25, step=0.05, continuous_update=False, description=r'$\\mu_{\\lambda_1}$'), \n",
    "            prior_y = wd.FloatSlider(value=0.0, min=-0.25, max=0.25, step=0.05, continuous_update=False, description=r'$\\mu_{\\lambda_2}$'),             \n",
    "            prior_std = wd.FloatSlider(value=1.0, min=0.5, max=2.0, step=0.05, continuous_update=False, description=r'$\\sigma_\\lambda$'), \n",
    "            data_std = wd.FloatSlider(value=0.5, min=0.05, max=1.0, step=0.01, continuous_update=False, description=r'$\\sigma_e$'), \n",
    "            trials = wd.IntSlider(value=10, min=1, max=500, step=1, continuous_update=False, description=r'trials'),\n",
    "            color_map = wd.Dropdown(value='viridis', options=colormaps, description=r'color map'), \n",
    "            num_levels = wd.IntSlider(value=20, min=10, max=50, step=5, continuous_update=False, description=r'colors'),\n",
    "            seed = wd.IntSlider(value=0, min=0, max=1000, step=1, continuous_update=False, description=r'seed', orientation='horizontal'),\n",
    "            show_accept = wd.Checkbox(description=\"Show Accepted\") )\n",
    "N, S, X, Y, prior_std, data_std, trials, color_map, colors, seed, show_accept, figure = out.children\n",
    "gui_2 = wd.VBox([trials, S, color_map, colors, seed])\n",
    "gui_1 = wd.VBox([N, X, Y, prior_std, data_std])\n",
    "gui = wd.HBox([gui_1, gui_2])\n",
    "# gui_list = wd.VBox([children[i] for i in range(9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99d1ed8acf04daeb89818096427286f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(IntSlider(value=1000, continuous_update=False, description='N', mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%capture output\n",
    "# # out = wd.interact(compare)\n",
    "box = wd.VBox([gui, figure])\n",
    "seed.value = 686 # move the seed to trigger a run (405, 214 misbehave, 108)\n",
    "box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so far the most consistent thing to integrate close to 1 is centering at the kahlman prediction and stochastic map. but as you up trials, the integration gets worse and worse. for some seeds it's awful. and for some seeds the hat shape comes back (i.e. 186, 95). does begin to lose hump with more trials, but also degenerates in integration. lowering prior variance can help but not by much (probably an artifact of the kde). \n",
    "\n",
    "\n",
    "deterministic model centered at noisy data has no bat ears. but it fails to integrate for a lot of seeds. looks a lot like kahlman solution in general. \n",
    "\n",
    "stochastic map centered at noisy data fails to integrate properly. \n",
    "\n",
    "deterministic model at kahlman (for completeness)... integrates to 1 plenty but is pure ring. nonsense. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deterministic Map\n",
    "\n",
    "(just as a sanity check) if you remove the noise from the model and feed it the true data, you DO get densities.\n",
    "\n",
    "This is because the MSE can be zero, so it more than covers the support. \n",
    "_However, the ring shape occurs consistently in this case. _\n",
    "\n",
    "If you feed the noisy data instead... you completely lose integrating to 1. the solution tracks the kahlman solution really well. No ring, but the predictability assumption is violated. \n",
    "\n",
    "If you feed it a LS estimate, it still integrates to 1 but it's a ring again. It adds mass in the direction the LS estimate needs to move to find the true value. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Stochastic Map - single noise realization\n",
    "\n",
    "Feeding **LS estimate**, the ring shape goes away, mass is still distributed towards truth, pushforward looks much better. But as trials increase, the mean of the ratio decreases. Increasing `N` does not resolve this. But lowering the variance of the prior does move the mean ratio in the right direction. \n",
    "\n",
    "Centering the prior at truth gets us over the 90% hump though ... _so this may be an artifact of random sampling_. \n",
    "HOWEVER... some seeds (e.g. 936) do annihilate the mean ratio and refuse to really resolve themselves. \n",
    "After further investigation, the mean of the observed samples is significantly biased away from truth (even after several hundred trials). Widening the variance can help, but we're still talking a mean ratio below 20%. \n",
    "\n",
    "** IT APPEARS `N` doesn't have much of an effect. **\n",
    "\n",
    "Some seeds we integrate to 1.2-ish... or 1.4, or 0.7 ... it's getting closer but it's still not there. \n",
    "Oddly enough, even when the sample mean of the observations is really close to truth, we still fail to integrate to 1 often enough! _As I change number of trials (holding all else constant), the integral of the posterior bounces all around. It's nuts._ Importantly, Changing the std of the data does not change the integral but does change the support of the posterior (or whatever that function is... since it's not a density). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j = np.linspace(1E-16, 20,1000)\n",
    "# k = sstats.distributions.chi2.pdf(j, 2)*0.5\n",
    "# plt.plot(j,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in progressbar.progressbar(list(range(1, 100, 1)) + list(range(100, 501, 10)) ):\n",
    "# for i in progressbar.progressbar(range(1000, 5001, 100) ):\n",
    "#     trials.value = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# convert -delay 8 skew/assimilating_pts_seed108_*.png seed108.mov\n",
    "# cp seed108.gif s108.gif\n",
    "# convert -delay 25 skew/assimilating_pts_seed262_*.png seed262.gif\n",
    "# cp seed262.gif s262.gif\n",
    "convert -delay 50 skew/finalassimilating_pts_seed405_*.png seed405.gif\n",
    "# convert -delay 12.5 skew/assimilating_pts_seed112_*.png seed112.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lucky experimentalist\n",
    "_(seed=262)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This person gets a posterior that evolves quite stably and converges rather quickly. So much so in fact that only 50 trials need to be performed before they are satisfied.\n",
    "\n",
    "![Seed 262 Results for 50 Trials](s262.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less lucky experimentalist\n",
    "_(seed=112)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a bit unlucky owing to the presence of outliers at low sample sizes. They have to double the sample size to get a similar amount of precision.\n",
    "\n",
    "![Seed 112 Results for 100 Trials](s112.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlucky Experimentalist\n",
    "_(seed=108)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to having a data set with a relatively large number of outliers, this person also was unable to observe quantity 2 directly, rather observing \n",
    "\n",
    "$$\\sqrt{3}\\lambda_1 + \\lambda_2,$$ \n",
    "\n",
    "a linear combination of the two variables instead. \n",
    "\n",
    "\n",
    "_They do not_, however, account for this correlation structure in defining their uncertainties, treating them as independent observations when imposing an observed distributions.\n",
    "\n",
    "So they have an unlucky experiment and collect more redundant information. Let us see how the two approaches based on the ConsistentBayes framework would approach identifying these parameters and their associated uncertainty.\n",
    "\n",
    "![Seed 108 Results for 100 Trials](https://imgur.com/a/HkNq8BZ.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias prior away from observation and lower its variance. Make the data very uncertain. \n",
    "The MSE approach handles this well, while error starts to pollute the vector-valued approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness (squared) acts like condition number (asymptotically) for this particular QoI map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewwrap(max_skew=2, res=1000, log=False):\n",
    "    skewness = np.linspace(1,max_skew,res)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    condition = []\n",
    "    for skew in skewness:\n",
    "        condition.append( np.linalg.cond(skewmat(skew)) )\n",
    "    plt.plot(skewness, condition, label='condition number')\n",
    "    plt.plot(skewness, skewness**2, label=r'skewness$^2$')\n",
    "    if log:\n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "    plt.ylabel('condition number',fontsize=18)\n",
    "    plt.xlabel('skewness',fontsize=18)\n",
    "    plt.title(\"skewness and condition number\",fontsize=24)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "wd.interactive(skewwrap, max_skew=wd.FloatSlider(min=1.1,max=10), res=wd.fixed(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,5, 100)\n",
    "mean = 2\n",
    "shft = 0.75\n",
    "plt.figure(figsize=(20,10))\n",
    "y = sstats.distributions.norm.pdf(x, loc=mean, scale=0.5)\n",
    "plt.plot(x,y, 'k')\n",
    "n = 5\n",
    "shftsum=0\n",
    "for shft in .5*np.random.randn(n):\n",
    "    y = sstats.distributions.norm.pdf(x, loc=mean+shft, scale=0.5)\n",
    "    plt.vlines(mean+shft,0,0.05,'r')\n",
    "    shftsum += shft\n",
    "meandata = shftsum/n + mean\n",
    "y = sstats.distributions.norm.pdf(x, loc=meandata, scale=0.5)\n",
    "plt.plot(x, y, 'r', alpha=0.5)\n",
    "plt.vlines(meandata,0,0.15,'r',lw=3, label='Observed Data')\n",
    "plt.vlines(mean,0,0.25,'k', label='Truth')\n",
    "plt.legend(fontsize=24)\n",
    "plt.savefig('better_observed.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
